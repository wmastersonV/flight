{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1> Hyper-parameter tuning </h1>\n",
    "\n",
    "In this notebook, you will learn how to carry out hyper-parameter tuning.\n",
    "\n",
    "This notebook takes several hours to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2> Environment variables for project and bucket </h2>\n",
    "\n",
    "Change the cell below to reflect your Project ID and bucket name. See Lab 3a for setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'just-aloe-200223' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'synergi' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-east1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "rm -rf taxifare.tar.gz flight_trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "# python -m trainer.task \\\n",
    "#   --train_data_paths=\"${PWD}/sample/train10.csv\" \\\n",
    "#   --eval_data_paths=${PWD}/sample/val10.csv  \\\n",
    "#   --output_dir=${PWD}/flight_trained \\\n",
    "#   --train_steps=10 \n",
    "# #  --job-dir=/tmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "airlines = [\"AS\", \"DL\", \"B6\", \"KH\", \"US\", \"MQ\", \"CO\", \"VX\", \"OO\", \"HA\", \"NW\", \"WN\", \"XE\", \"AA\", \"FL\", \"UA\", \"YV\", \"EV\", \"OH\", \"F9\", \"TZ\", \"9E\", \"DH\", \"HP\"]\n",
    "depart_air = [\"ONT\", \"RNO\", \"GST\", \"PSG\", \"KTN\", \"HPN\", \"ISP\", \"GCC\", \"AEX\", \"TUS\", \"BOI\", \"BIL\", \"LIH\", \"MRY\", \"SDF\", \"SBA\", \"OKC\", \"TLH\", \"WRG\", \"VIS\", \"TUL\", \"STT\", \"EAU\", \"RDM\", \"GRR\", \"TYS\", \"CAK\", \"PSP\", \"ITO\", \"HLN\", \"SCC\", \"CDV\", \"MLI\", \"JAC\", \"STX\", \"BLI\", \"BRW\", \"MFR\", \"MAZ\", \"OME\", \"MAF\", \"MKG\", \"PSC\", \"JAN\", \"CRP\", \"MEI\", \"CDC\", \"INL\", \"PNS\", \"ESC\", \"LWB\", \"GSP\", \"PFN\", \"GPT\", \"PIR\", \"AUS\", \"BNA\", \"BWI\", \"CLE\", \"CLT\", \"DFW\", \"DTW\", \"EWR\", \"FLL\", \"HNL\", \"IAD\", \"IAH\", \"IND\", \"LAS\", \"LAX\", \"LGA\", \"MCI\", \"MIA\", \"OAK\", \"ORD\", \"PDX\", \"RDU\", \"TPA\", \"ALB\", \"MHT\", \"DAY\", \"AMA\", \"BFL\", \"GNV\", \"CHS\", \"SAV\", \"EYW\", \"GSO\", \"EGE\", \"OTH\", \"PWM\", \"MYR\", \"AVP\", \"BTV\", \"SGF\", \"CID\", \"ICT\", \"DAB\", \"CRW\", \"MMH\", \"CHA\", \"CAE\", \"VPS\", \"ART\", \"LNK\", \"MBS\", \"LSE\", \"ASE\", \"MTJ\", \"ORH\", \"SPI\", \"AZO\", \"DLH\", \"SUX\", \"RST\", \"CHO\", \"GJT\", \"FWA\", \"CMX\", \"CMI\", \"AVL\", \"CPR\", \"PSE\", \"GUC\", \"CLL\", \"MLB\", \"FSM\", \"DRO\", \"SJT\", \"TXK\", \"LAW\", \"BRO\", \"ILE\", \"MVY\", \"BQN\", \"ABR\", \"ISN\", \"MOT\", \"GFK\", \"ACY\", \"ILG\", \"DHN\", \"VLD\", \"ISO\", \"TTN\", \"TUP\", \"APF\", \"HTS\", \"SOP\", \"OAJ\", \"FLO\", \"MCN\", \"MTH\", \"EWN\", \"HKY\", \"PLN\", \"EFD\", \"VCT\", \"HVN\", \"GUM\", \"MWH\", \"SLE\", \"SUN\", \"WYS\", \"MOD\", \"RDD\", \"PMD\", \"LAR\", \"OXR\", \"SMX\", \"PUB\", \"CKB\", \"FMN\", \"PVU\", \"BFF\", \"RKS\", \"OGG\", \"RSW\", \"PBI\", \"JAX\", \"CEC\", \"ANC\", \"BDL\", \"SJU\", \"PVD\", \"SYR\", \"MLU\", \"FAT\", \"MKE\", \"BET\", \"LNY\", \"ADK\", \"MSO\", \"BHM\", \"LFT\", \"HIB\", \"GTF\", \"GEG\", \"KOA\", \"FNT\", \"BUF\", \"ELP\", \"SWF\", \"OMA\", \"RIC\", \"ORF\", \"SIT\", \"YAK\", \"JNU\", \"LGB\", \"EKO\", \"COS\", \"ROC\", \"CWA\", \"LBB\", \"AKN\", \"IMT\", \"SGU\", \"BMI\", \"OTZ\", \"MKK\", \"ACV\", \"FAI\", \"ROA\", \"EVV\", \"ADQ\", \"DLG\", \"GTR\", \"PIE\", \"BGR\", \"ILM\", \"BTR\", \"PIA\", \"DUT\", \"SHV\", \"HSV\", \"ABQ\", \"ATL\", \"BOS\", \"CMH\", \"CVG\", \"DAL\", \"DCA\", \"DEN\", \"HOU\", \"JFK\", \"MCO\", \"MDW\", \"MEM\", \"MSP\", \"MSY\", \"PHL\", \"PHX\", \"PIT\", \"SAN\", \"SAT\", \"SEA\", \"SFO\", \"SJC\", \"SLC\", \"SMF\", \"SNA\", \"STL\", \"BUR\", \"LIT\", \"FSD\", \"SBP\", \"MSN\", \"DSM\", \"FAR\", \"EUG\", \"XNA\", \"LMT\", \"MDT\", \"ABE\", \"ERI\", \"SRQ\", \"SBN\", \"LEX\", \"GRB\", \"IPL\", \"TVC\", \"PAH\", \"MOB\", \"BIS\", \"LAN\", \"ATW\", \"ALO\", \"TRI\", \"TOL\", \"RAP\", \"BZN\", \"MHK\", \"DBQ\", \"HDN\", \"HRL\", \"PHF\", \"ACT\", \"ABI\", \"SAF\", \"FAY\", \"AGS\", \"BPT\", \"GGG\", \"SPS\", \"ROW\", \"MGM\", \"CSG\", \"CYS\", \"TYR\", \"LRD\", \"GRK\", \"GRI\", \"MFE\", \"JLN\", \"GCK\", \"LCH\", \"BGM\", \"ACK\", \"IDA\", \"RHI\", \"APN\", \"BRD\", \"BJI\", \"LYH\", \"BQK\", \"COU\", \"ABY\", \"ITH\", \"ELM\", \"HOB\", \"DRT\", \"BTM\", \"TWF\", \"LWS\", \"COD\", \"PIH\", \"YKM\", \"OGD\", \"CIC\", \"RFD\", \"IYK\", \"FLG\", \"TEX\", \"SHD\", \"MKC\", \"GLH\"]\n",
    "\n",
    "arrival_air = [\"GCC\", \"ISP\", \"BOI\", \"JAN\", \"VIS\", \"PNS\", \"SBA\", \"MEI\", \"INL\", \"VLD\", \"ITO\", \"PSP\", \"ONT\", \"TUL\", \"KTN\", \"RNO\", \"MFR\", \"EAU\", \"LIH\", \"MRY\", \"MAF\", \"ESC\", \"CDC\", \"OME\", \"TLH\", \"STX\", \"HPN\", \"BIL\", \"BRW\", \"GRR\", \"LWB\", \"OKC\", \"DAY\", \"CAK\", \"GPT\", \"TUS\", \"WRG\", \"SGF\", \"STT\", \"SCC\", \"SDF\", \"GST\", \"AEX\", \"HLN\", \"JAC\", \"BTV\", \"PSG\", \"CDV\", \"MKG\", \"MCN\", \"BRO\", \"MLI\", \"AVL\", \"MAZ\", \"SAV\", \"GSO\", \"PFN\", \"ALB\", \"PWM\", \"TUP\", \"TYS\", \"ICT\", \"ILG\", \"CRP\", \"LAW\", \"SOP\", \"TTN\", \"ASE\", \"MTJ\", \"PSE\", \"AVP\", \"DAB\", \"APF\", \"CHO\", \"GNV\", \"VPS\", \"EWN\", \"GSP\", \"DHN\", \"EGE\", \"MHT\", \"MTH\", \"ACY\", \"FWA\", \"CHA\", \"CID\", \"ISO\", \"EYW\", \"FLO\", \"LNK\", \"HKY\", \"FSM\", \"CRW\", \"OAJ\", \"CAE\", \"MLB\", \"HTS\", \"AZO\", \"BQN\", \"CMI\", \"HVN\", \"RDM\", \"LAR\", \"BLI\", \"BFL\", \"DRO\", \"GUC\", \"PSC\", \"GJT\", \"GFK\", \"ISN\", \"MOT\", \"CPR\", \"SJT\", \"TXK\", \"ILE\", \"CLL\", \"SPI\", \"RST\", \"DLH\", \"LSE\", \"MBS\", \"PLN\", \"GUM\", \"EFD\", \"VCT\", \"MVY\", \"OXR\", \"SMX\", \"MOD\", \"ABR\", \"PIR\", \"SUX\", \"CMX\", \"ART\", \"ORH\", \"OTH\", \"MMH\", \"MWH\", \"RDD\", \"PMD\", \"SUN\", \"WYS\", \"SLE\", \"LAS\", \"MIA\", \"PDX\", \"CLE\", \"ORD\", \"FLL\", \"DFW\", \"CHS\", \"OAK\", \"AUS\", \"DTW\", \"CLT\", \"AMA\", \"IND\", \"IAH\", \"MCI\", \"BNA\", \"HNL\", \"RDU\", \"EWR\", \"IAD\", \"TPA\", \"LAX\", \"LGA\", \"BWI\", \"MYR\", \"CKB\", \"CBM\", \"PVU\", \"KOA\", \"ACV\", \"ADQ\", \"AKN\", \"ORF\", \"PBI\", \"JNU\", \"BDL\", \"PVD\", \"RKS\", \"OMA\", \"ROC\", \"GTR\", \"SRQ\", \"MKK\", \"GTF\", \"HSV\", \"MSO\", \"BET\", \"COS\", \"RSW\", \"LGB\", \"BHM\", \"LBB\", \"RIC\", \"SHV\", \"FAT\", \"OGG\", \"SJU\", \"CWA\", \"SYR\", \"DLG\", \"EKO\", \"BTR\", \"DSM\", \"GRB\", \"LEX\", \"GEG\", \"ATW\", \"FNT\", \"YAK\", \"ANC\", \"MGM\", \"RAP\", \"DUT\", \"BMI\", \"COD\", \"ADK\", \"FAI\", \"SIT\", \"OTZ\", \"MSN\", \"EUG\", \"ELP\", \"JAX\", \"MLU\", \"SWF\", \"PIE\", \"LFT\", \"SGU\", \"HIB\", \"PHF\", \"IMT\", \"ROA\", \"BUR\", \"XNA\", \"PIA\", \"AGS\", \"LAN\", \"MDT\", \"BGM\", \"TVC\", \"FSD\", \"CSG\", \"BPT\", \"BGR\", \"COU\", \"LIT\", \"BQK\", \"ABY\", \"LYH\", \"MFE\", \"BZN\", \"TRI\", \"ILM\", \"ERI\", \"ABE\", \"HDN\", \"FAY\", \"MOB\", \"SBN\", \"EVV\", \"TOL\", \"GRK\", \"HRL\", \"ACK\", \"RFD\", \"BIS\", \"FAR\", \"IDA\", \"GGG\", \"ABI\", \"SPS\", \"ROW\", \"GCK\", \"CYS\", \"LCH\", \"JLN\", \"ACT\", \"MHK\", \"LRD\", \"TYR\", \"GRI\", \"SAF\", \"ITH\", \"APN\", \"ELM\", \"LNY\", \"HOB\", \"DRT\", \"SBP\", \"IYK\", \"IPL\", \"SHD\", \"BRD\", \"ALO\", \"BJI\", \"RHI\", \"DBQ\", \"PAH\", \"LMT\", \"TEX\", \"FLG\", \"CIC\", \"TWF\", \"YKM\", \"LWS\", \"BTM\", \"PIH\", \"PIT\", \"STL\", \"CMH\", \"SAN\", \"MSY\", \"SAT\", \"JFK\", \"MSP\", \"ATL\", \"MEM\", \"ABQ\", \"CVG\", \"PHL\", \"SEA\", \"SLC\", \"HOU\", \"MDW\", \"CEC\", \"BOS\", \"SJC\", \"SFO\", \"DEN\", \"SMF\", \"MKE\", \"BUF\", \"DCA\", \"PHX\", \"MCO\", \"DAL\", \"SNA\", \"GLH\", \"MKC\"]\n",
    "all_air = list(set(depart_air + arrival_air))\n",
    "\n",
    "CSV_COLUMNS = ['departure_lat', 'departure_lon', 'arrival_lat', 'arrival_lon', 'airline', 'departure_airport','arrival_airport','dow','week','month', \\\n",
    "               'arrival_delay', 'delay_0', 'delay_15', 'delay_30', 'delay_45', 'delay_60', 'depart_minutes', 'scheduled_flight_time']\n",
    "\n",
    "LABEL_COLUMN = 'arrival_delay'\n",
    "\n",
    "DEFAULTS = [[999.0], [999.0], [999.0], [999.0], ['NA'], ['NA'],['NA'], [999], [999], [999], [6], [2], [2], [2], [2], [2], [807], [111]]\n",
    "\n",
    "# These are the raw input columns, and will be provided for prediction also\n",
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.categorical_column_with_identity('week', num_buckets=54),\n",
    "    tf.feature_column.categorical_column_with_identity('dow', num_buckets = 8),\n",
    "    tf.feature_column.categorical_column_with_identity('month', num_buckets = 13),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('airline', vocabulary_list=airlines),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('arrival_airport',vocabulary_list=all_air),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('departure_airport',vocabulary_list=all_air),\n",
    "    tf.feature_column.numeric_column('depart_minutes'),\n",
    "    tf.feature_column.numeric_column('scheduled_flight_time'),\n",
    "    tf.feature_column.numeric_column('departure_lat'),\n",
    "    tf.feature_column.numeric_column('departure_lon'),\n",
    "    tf.feature_column.numeric_column('arrival_lat'),\n",
    "    tf.feature_column.numeric_column('arrival_lon'),\n",
    "    \n",
    "#    engineered features\n",
    "    tf.feature_column.numeric_column('latdiff'),\n",
    "    tf.feature_column.numeric_column('londiff'),\n",
    "    tf.feature_column.numeric_column('euclidean')\n",
    "]\n",
    "\n",
    "\n",
    "# Build the estimator\n",
    "def build_estimator(model_dir, nbuckets, hidden_units):\n",
    "    \"\"\"\n",
    "     Build an estimator starting from INPUT COLUMNS.\n",
    "     These include feature transformations and synthetic features.\n",
    "     The model is a wide-and-deep model.\n",
    "  \"\"\"\n",
    "\n",
    "    (week, dow, month, airline, arrival_airport, departure_airport, depart_minutes, scheduled_flight_time, departure_lat, departure_lon, \\\n",
    "    arrival_lat, arrival_lon, latdiff, londiff, euclidean) = INPUT_COLUMNS\n",
    "#    depart_minutes = tft.scale_to_0_1(depart_minutes)\n",
    "#    scheduled_flight_time = tft.scale_to_0_1(scheduled_flight_time)    \n",
    "#    departure_lat = tft.scale_to_0_1(departure_lat)\n",
    "#    departure_lon = tft.scale_to_0_1(departure_lon)   \n",
    "#    arrival_lat = tft.scale_to_0_1(arrival_lat)\n",
    "#    arrival_lon = tft.scale_to_0_1(arrival_lon)  \n",
    "#    depart_minutes = tft.scale_to_0_1(depart_minutes)\n",
    "#    scheduled_flight_time = tft.scale_to_0_1(scheduled_flight_time)\n",
    "    \n",
    "    # Bucketize the lats & lons\n",
    "    latbuckets = np.linspace(-180, 180, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(-180, 180, nbuckets).tolist()\n",
    "    b_plat = tf.feature_column.bucketized_column(departure_lat, latbuckets)\n",
    "#    print(b_plat)\n",
    "    b_dlat = tf.feature_column.bucketized_column(arrival_lat, latbuckets)\n",
    "    b_plon = tf.feature_column.bucketized_column(departure_lon, lonbuckets)\n",
    "    b_dlon = tf.feature_column.bucketized_column(arrival_lon, lonbuckets)\n",
    "\n",
    "    # Feature cross\n",
    "\n",
    "    ploc = tf.feature_column.crossed_column([b_plat, b_plon], 100 * 100)\n",
    "    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], 100 * 100)\n",
    "    pd_pair = tf.feature_column.crossed_column([ploc, dloc], 100 ** 4 )\n",
    "#    day_hr =  tf.feature_column.crossed_column([dow, tf.floor(depart_minutes/60)], 24 * 7)\n",
    "\n",
    "    # Wide columns and deep columns.\n",
    "    wide_columns = [\n",
    "        # Feature crosses\n",
    "        dloc, ploc, pd_pair,\n",
    "        #day_hr,\n",
    "\n",
    "        # Sparse columns\n",
    "        week, dow, month, airline \n",
    "        #arrival_airport, departure_airport\n",
    "\n",
    "        # Anything with a linear relationship\n",
    "#        pcount \n",
    "    ]\n",
    "\n",
    "    deep_columns = [\n",
    "        # Embedding_column to \"group\" together ...\n",
    "        tf.feature_column.embedding_column(pd_pair, 10),\n",
    "#        tf.feature_column.embedding_column(day_hr, 10),\n",
    "\n",
    "        # Numeric columns\n",
    "        depart_minutes, scheduled_flight_time, departure_lat, departure_lon,\n",
    "        arrival_lat, arrival_lon,\n",
    "        latdiff, londiff, euclidean\n",
    "    ]\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir = model_dir,\n",
    "        linear_feature_columns = wide_columns,\n",
    "        dnn_feature_columns = deep_columns,\n",
    "        dnn_hidden_units = hidden_units or [128, 32, 4])\n",
    "\n",
    "    # add extra evaluation metric for hyperparameter tuning\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, add_eval_metrics)\n",
    "    return estimator\n",
    "\n",
    "# Create feature engineering function that will be used in the input and serving input functions\n",
    "def add_engineered(features):\n",
    "    # this is how you can do feature engineering in TensorFlow\n",
    "    lat1 = features['departure_lat']\n",
    "    lat2 = features['arrival_lat']\n",
    "    lon1 = features['departure_lon']\n",
    "    lon2 = features['arrival_lon']\n",
    "    latdiff = (lat1 - lat2)\n",
    "    londiff = (lon1 - lon2)\n",
    "    \n",
    "    # set features for distance with sign that indicates direction\n",
    "    features['latdiff'] = latdiff\n",
    "    features['londiff'] = londiff\n",
    "    dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
    "    features['euclidean'] = dist\n",
    "    return features\n",
    "\n",
    "# Create serving input function to be able to serve predictions\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        # All the real-valued columns\n",
    "        column.name: tf.placeholder(tf.float32, [None]) for column in INPUT_COLUMNS[2:]\n",
    "    }\n",
    "    feature_placeholders['dayofweek'] = tf.placeholder(tf.string, [None])\n",
    "    feature_placeholders['hourofday'] = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(add_engineered(features), feature_placeholders)\n",
    "\n",
    "# Create input function to load data into datasets\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return add_engineered(features), label\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(filename)\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        batch_features, batch_labels = dataset.make_one_shot_iterator().get_next()\n",
    "        return batch_features, batch_labels\n",
    "    return _input_fn\n",
    "\n",
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(args):\n",
    "    estimator = build_estimator(args['output_dir'], args['nbuckets'], args['hidden_units'].split(' '))\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args['train_data_paths'],\n",
    "            mode = tf.estimator.ModeKeys.TRAIN,\n",
    "            batch_size = args['train_batch_size']),\n",
    "        max_steps = args['train_steps'])\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args['eval_data_paths'],\n",
    "            mode = tf.estimator.ModeKeys.EVAL,\n",
    "            batch_size = args['eval_batch_size']),\n",
    "        steps = 100,\n",
    "        exporters = exporter)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "# If we want to use TFRecords instead of CSV\n",
    "def gzip_reader_fn():\n",
    "    return tf.TFRecordReader(options=tf.python_io.TFRecordOptions(\n",
    "            compression_type = tf.python_io.TFRecordCompressionType.GZIP))\n",
    "\n",
    "def generate_tfrecord_input_fn(data_paths, num_epochs = None, batch_size = 512, mode = tf.estimator.ModeKeys.TRAIN):\n",
    "    def get_input_features():\n",
    "        # Read the tfrecords. Same input schema as in preprocess\n",
    "        input_schema = {}\n",
    "        if mode != tf.estimator.ModeKeys.INFER:\n",
    "            input_schema[LABEL_COLUMN] = tf.FixedLenFeature(shape = [1], dtype = tf.float32, default_value = 0.0)\n",
    "        for name in ['dayofweek', 'key']:\n",
    "            input_schema[name] = tf.FixedLenFeature(shape = [1], dtype = tf.string, default_value = 'null')\n",
    "        for name in ['hourofday']:\n",
    "            input_schema[name] = tf.FixedLenFeature(shape = [1], dtype = tf.int64, default_value = 0)\n",
    "        for name in SCALE_COLUMNS:\n",
    "            input_schema[name] = tf.FixedLenFeature(shape = [1], dtype = tf.float32, default_value = 0.0)\n",
    "\n",
    "        # How? \n",
    "        keys, features = tf.contrib.learn.io.read_keyed_batch_features(\n",
    "            data_paths[0] if len(data_paths) == 1 else data_paths,\n",
    "            batch_size,\n",
    "            input_schema,\n",
    "            reader = gzip_reader_fn,\n",
    "            reader_num_threads = 4,\n",
    "            queue_capacity = batch_size * 2,\n",
    "            randomize_input = (mode != tf.estimator.ModeKeys.EVAL),\n",
    "            num_epochs = (1 if mode == tf.estimator.ModeKeys.EVAL else num_epochs))\n",
    "        target = features.pop(LABEL_COLUMN)\n",
    "        features[KEY_FEATURE_COLUMN] = keys\n",
    "        return add_engineered(features), target\n",
    "\n",
    "    # Return a function to input the features into the model from a data path.\n",
    "    return get_input_features\n",
    "\n",
    "def add_eval_metrics(labels, predictions):\n",
    "    pred_values = predictions['predictions']\n",
    "    return {\n",
    "        'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
